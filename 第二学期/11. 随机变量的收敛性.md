
#### 1. 逐点收敛和一致收敛

设 $(f_n)$ 是一个函数列，$f_n: M \to \mathbb{R}$，$\emptyset \neq M \subseteq \mathbb{R}$。

a) $(f_n)$ 逐点收敛于 $f: M \to \mathbb{K}$ 当且仅当  
对于所有 $z \in M$，$\lim_{n \to \infty} f_n(z) = f(z)$，即

对于所有 $z \in M$，对于所有 $\epsilon \in \mathbb{R}^+$，存在 $N \in \mathbb{N}$，使得对于所有 $n > N$，$|f_n(z) - f(z)| < \epsilon$

$N$ 通常取决于 $z$ 和 $\epsilon$。

b) $(f_n)$ 一致收敛于 $f: M \to \mathbb{K}$ 当且仅当

对于所有 $\epsilon \in \mathbb{R}^+$，存在 $N \in \mathbb{N}$，使得对于所有 $n > N$，对于所有 $z \in M$，$|f_n(z) - f(z)| < \epsilon$

$N$ 不依赖于 $z$，但通常仍然依赖于 $\epsilon$ → 收敛对于 $z$ 是一致的。

**一致收敛可以推出逐点收敛，但反之则不成立。**

#### 2. 收敛类型

▶ 几乎必然收敛（几乎确定收敛）  
▶ 期望收敛  
▶ 概率收敛  
▶ 分布收敛（将在下一章详细讨论）

#### 3. 几乎必然收敛

![[29f5583b-d5e2-4d0a-9dd5-91e4bd45847e.png]]

#### 4. 概率收敛
![[9fed8637-a1fc-484b-818f-1571ffb512dc.png]]
![[c97e854f-7b8b-4765-b0df-1a5b293ee01b.png]]

- **如果能证明几乎必然收敛，则概率收敛自动成立**
#### 5. 矩收敛

![[21fb49db-2829-4615-99a4-9516d14e959a.png]]
#### 6. 独立同分布
![[750c468c-3448-4afc-8eec-573f4c51701c.png]]
#### 7. 弱大数定理
![[80c88ade-fbcc-48f2-b19f-8ddc3e8af193.png]]
![[ee63cc10-870c-4897-8169-6bd77d72ac9f.png]]
- **只要满足独立同分布且有有限期望值的条件，弱大数定律就适用**
#### 8. 伯努利定理


#### 9. 辛钦定理
![[1875c321-504d-4db0-a353-8fb187c59c0f.png]]
- **辛钦定理表明大数定律的本质在于期望值的存在，而非方差的有限性**
- **为处理重尾分布数据提供了理论支持**
- **柯西分布没有期望值,因此不适用弱大数定理**
#### 10. 大数定律的反例与强大数定律
![[f489fa52-baa6-4e7a-b44e-df7d860beedd.png]]
- $\mathbb{P}(\lim_{n\to\infty} \bar{X}_n = \mathbb{E}(X_1)) = 1$
- **弱大数定律**：样本均值"很可能"接近期望值（依概率收敛）
- **强大数定律**：样本均值"几乎肯定"最终会收敛到期望值（几乎必然收敛）




#### 11. 蒙特卡洛方法
![[df8d889b-3176-448c-90c2-5bd6e46e1de5.png]]
### 局限性

- 需要**大量随机点**才能获得高精度结果
- **结果是近似的**，有随机误差
- **随着维度增加，需要的点数呈指数增长**
#### 12. 一般大数定律
![[f8682397-fc13-418b-9884-b92e2e7d44f2.png]]
![[e6251bca-ce5a-45fd-87e2-8aba41acf5cc.png]]

![[a04203aa-a8ed-4b1b-992b-2f38d15d6f71.png]]
![[035d1f83-9d6b-42c6-a3ae-5306a1b583d9.png]]
![[a9347cde-ea15-495d-bc29-ade55ab9170f.png]]
![[ce844377-0c6e-479a-a832-a07c5f895ebc.png]]









### 1. 标准大数定律（柯尔莫戈洛夫第二定理）

- **条件**：独立同分布随机变量序列，有限期望值
- **结论**：样本均值几乎必然收敛到期望值
- **形式**：$\lim_{n\to\infty} \bar{X}_n = \mathbb{E}(X_1) \quad \mathbb{P}-f.s.$
- **特点**：最经典、应用最广泛的版本

### 2. 埃特马迪定理

- **条件**：同分布且两两独立，有限期望值
- **结论**：样本均值几乎必然收敛到期望值
- **形式**：同标准大数定律
- **特点**：放宽了完全独立性要求，**只需两两独立**

### 3. 柯尔莫戈洛夫第一定理

- **条件**：独立随机变量序列，有限方差，满足$\sum_{n=1}^{\infty} \frac{\text{Var}(X_n)}{n^2} < \infty$
- **结论**：中心化样本均值几乎必然收敛到0
- **形式**：$\lim_{n\to\infty} \frac{1}{n}\sum_{i=1}^{n}(X_i - \mathbb{E}(X_i)) = 0 \quad \mathbb{P}-f.s.$
- **特点**：放宽了同分布要求，但要求方差序列满足特定条件

### 4. 坎特利定理

- **条件**：独立随机变量序列，**有限四阶矩**，中心化四阶矩一致有界
- **结论**：中心化样本均值几乎必然收敛到0
- **形式**：同柯尔莫戈洛夫第一定理
- **特点**：通过四阶矩条件控制波动，是另一种处理非同分布情况的方法

### 5. 一般大数定律（定义11.12）

- **条件**：随机变量序列，有限期望值
- **结论**：中心化样本均值收敛到0（强收敛或弱收敛）
- **形式**：$\bar{X}_n := \frac{1}{n}\sum_{i=1}^{n}(X_i - \mathbb{E}(X_i)) \to 0$
- **特点**：提供了大数定律的一般框架定义，不特定假设独立性或同分布

## 二、条件强弱比较

### 独立性条件（从强到弱）

1. **完全独立**：任何子集合的随机变量都相互独立（柯尔莫戈洛夫定理）
2. **两两独立**：仅要求任意两个随机变量之间相互独立（埃特马迪定理）
3. **无独立性要求**：一般大数定律定义不直接要求独立性

### 分布条件（从强到弱）

1. **同分布**：所有随机变量服从相同分布（标准大数定律、埃特马迪定理）
2. **非同分布但有特定方差条件**：柯尔莫戈洛夫第一定理
3. **非同分布但有四阶矩条件**：坎特利定理
4. **无分布一致性要求**：一般大数定律定义

### 矩条件（从强到弱）




- **分布收敛**：当 $n \to \infty$ 时，$F_{X_n}(x) \to F_X(x)$，随着练习，他的箭的落点分布形状越来越接近某个特定模式，但这个模式可能不总是以靶心为中心
- **概率收敛**：对任何 $\varepsilon > 0$，$\lim_{n \to \infty} P(|X_n - X| > \varepsilon) = 0$，随着练习，他射出远离靶心的箭的几率越来越小
- **期望收敛**：$\lim_{n \to \infty} E[(X_n - X)^2] = 0$，随着练习，他的箭与靶心的平均距离平方越来越小
- **必然收敛**：$P(\lim_{n \to \infty} X_n = X) = 1$，经过足够长时间的练习后，他的每一箭都会越来越稳定地落在靶心附近的同一区域
- **收敛强度**：几乎必然收敛 → 均方收敛 → 概率收敛 → 分布收敛

## 分布收敛：形状逐渐相似

**定义**：随机变量序列 $X_n$ 按分布收敛到 $X$，如果对任何连续函数 $F_X$，当 $n \to \infty$ 时，$F_{X_n}(x) \to F_X(x)$。

**直观理解**：想象每个 $X_n$ 是一座山的轮廓。按分布收敛意味着随着 $n$ 增大，山的形状越来越接近目标山 $X$ 的形状。但重要的是：我们并不关心山在哪里，只关心它的形状是否相似。

如果你把每个 $X_n$ 的概率分布画出来，随着 $n$ 增大，这些曲线会越来越接近 $X$ 的分布曲线。这就像是一个雕塑家逐渐完善他的作品，直到最终形态。

## 概率收敛：聚焦的镜头

**定义**：随机变量序列 $X_n$ 依概率收敛到 $X$，如果对任何 $\varepsilon > 0$，  
$\lim_{n \to \infty} P(|X_n - X| > \varepsilon) = 0$。

**直观理解**：想象你通过一个逐渐聚焦的相机观察 $X_n$ 和 $X$ 之间的差距。起初，相机模糊不清，你看到的差距可能很大。但随着 $n$ 增大，相机越来越聚焦，你能看到的显著差距越来越少。

在概率收敛中，我们不要求每次实验都必须接近目标值，而是要求"出现明显偏差"的概率变得越来越小。这就像是投掷一个逐渐被加重的骰子，它越来越倾向于显示特定的点数。

## 期望收敛（均方收敛）：能量衰减

**定义**：随机变量序列 $X_n$ 依均方收敛到 $X$，如果  
$\lim_{n \to \infty} E[(X_n - X)^2] = 0$。

**直观理解**：想象 $X_n$ 和 $X$ 之间的差距是一种"能量"。期望收敛要求这种能量的平均值随着 $n$ 增大而衰减至零。

与概率收敛不同，期望收敛对偏差的惩罚更为严厉——偏差被平方了！这意味着大偏差会受到特别严重的惩罚。这就像是一个物理系统，我们不仅要求它大部分时间接近平衡状态，还要求它的总能量波动逐渐减少。

## 几乎必然收敛：长久的稳定

**定义**：随机变量序列 $X_n$ 几乎必然收敛到 $X$，如果  
$P(\lim_{n \to \infty} X_n = X) = 1$。

**直观理解**：这是最强的收敛形式。想象无数条可能的轨迹，每条代表随机变量序列的一种可能演化。几乎必然收敛意味着，这些轨迹中的几乎所有最终都会稳定在目标值附近，并永远留在那里。

这就像是将一个小球放在一个有许多凹槽的表面上轻轻摇晃。起初，球可能四处跳动，但随着时间推移，它几乎肯定会落入某个特定的凹槽并停留在那里。

## 收敛强度比较

这四种收敛方式按照强度排序为：

几乎必然收敛 → 均方收敛 → 概率收敛 → 分布收敛

这种层次关系可以这样理解：

- 几乎必然收敛意味着"轨迹层面"的收敛
- 均方收敛关注"能量层面"的收敛
- 概率收敛关注"偏差概率"的收敛
- 分布收敛仅关注"外观形状"的收敛

## 直观例子：射箭比赛

想象一个不断练习的射箭手：

- **分布收敛**：随着练习，他的箭的落点分布形状越来越接近某个特定模式，但这个模式可能不总是以靶心为中心。
- **概率收敛**：随着练习，他射出远离靶心的箭的几率越来越小。
- **均方收敛**：随着练习，他的箭与靶心的平均距离平方越来越小。
- **几乎必然收敛**：经过足够长时间的练习后，他的每一箭都会越来越稳定地落在靶心附近的同一区域。

这些不同的收敛概念为我们提供了理解随机过程如何"稳定"的不同视角，每一种都有其独特的数学意义和实际应用场景。


求 X₂ 的边缘密度函数 f(x₂)，我们需要对联合密度函数 f(x₁, x₂) 关于 x₁ 进行积分。

给定的联合密度函数为：

$\begin{cases} \lambda \exp(-\sqrt{\lambda}(x_1 + x_2)) & \text{für } x_1 > 0 \text{ und } x_2 > 0, \\ 0 & \text{sonst}. \end{cases}$ X₂ 的边缘密度函数计算如下： $f_{X_2}(x_2) = \int_{-\infty}^{\infty} f(x_1, x_2) \, dx_1$ 由于 f(x₁, x₂) 在 x₁ ≤ 0 时为零，我们可以将积分区间改为： $$f_{X_2}(x_2) = \int_{0}^{\infty} f(x_1, x_2) \, dx_1$$ 将联合密度函数代入（当 x₂ > 0 时）： $$f_{X_2}(x_2) = \int_{0}^{\infty} \lambda \exp(-\sqrt{\lambda}(x_1 + x_2)) \, dx_1$$ 提取与 x₁ 无关的部分： $$f_{X_2}(x_2) = \lambda \exp(-\sqrt{\lambda}x_2) \int_{0}^{\infty} \exp(-\sqrt{\lambda}x_1) \, dx_1$$ 计算积分： $$\int_{0}^{\infty} \exp(-\sqrt{\lambda}x_1) \, dx_1 = \left[ \frac{-1}{\sqrt{\lambda}} \exp(-\sqrt{\lambda}x_1) \right]_{0}^{\infty} = \frac{1}{\sqrt{\lambda}}$$ 因此： $$f_{X_2}(x_2) = \lambda \exp(-\sqrt{\lambda}x_2) \cdot \frac{1}{\sqrt{\lambda}} = \sqrt{\lambda} \exp(-\sqrt{\lambda}x_2)$$ 当 x₂ ≤ 0 时，f_{X₂}(x₂) = 0。 所以，X₂ 的边缘密度函数为： $$f_{X_2}(x_2) = \begin{cases} \sqrt{\lambda} \exp(-\sqrt{\lambda}x_2) & \text{für } x_2 > 0, \\ 0 & \text{sonst}. \end{cases}$$ 这是指数分布的密度函数，参数为 $\sqrt{\lambda}$。


这涉及到指示函数（indicator function）的性质和转换。让我解释一下这两个指示函数的含义和它们之间的变换关系：

1. $I_{(0,\infty)}(y - x_2)$ 表示当 $y - x_2 > 0$ 时函数值为1，否则为0。这等价于 $y > x_2$。
    
2. $I_{(0,y)}(x_2)$ 表示当 $0 < x_2 < y$ 时函数值为1，否则为0。
    

这两个条件实际上是等价的：

- $I_{(0,\infty)}(y - x_2) = 1$ 当且仅当 $y - x_2 > 0$，即 $y > x_2$
- $I_{(0,y)}(x_2) = 1$ 当且仅当 $0 < x_2 < y$

当我们考虑到 $x_2 > 0$（根据原问题的联合密度函数的定义域），这两个条件变为等价：

- $y > x_2 > 0$ 等价于 $0 < x_2 < y$

因此，在给定的积分区域内（即 $x_2 > 0$）：  
$I_{(0,\infty)}(y - x_2) = I_{(0,y)}(x_2)$

I(0,∞)​(y−x2​)=I(0,y)​(x2​)

这是积分变量转换过程中的一步，它帮助我们将原始的条件重新表述为一个更方便进行积分的形式。